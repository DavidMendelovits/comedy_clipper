# Comedy Clipper - Unified Configuration
# Edit this file to customize how the clipper detects comedian sets
# Supports multiple detection modes configurable through 'detection_mode'

# ============================================================================
# DETECTION MODE
# ============================================================================
# Choose which detection method to use:
#   multimodal   - Face + pose detection (best for standup with hosts)
#   pose         - Pose detection only
#   face         - Face detection only
#   mediapipe    - MediaPipe pose tracking (entry/exit detection)
#   scene        - FFmpeg scene detection (camera cuts)
#   diarization  - Speaker diarization (requires HF_TOKEN env variable)

detection_mode: multimodal

# ============================================================================
# SCENE DETECTION (for mode: scene)
# ============================================================================
# Detects video cuts between comedians

scene_detection:
  # Scene change sensitivity (0.0-1.0, lower = more sensitive)
  threshold: 0.3

  # Maximum gap between scenes to group as one segment (seconds)
  max_gap: 5.0

# ============================================================================
# TRANSITION-BASED DETECTION (for modes: multimodal, pose, face, mediapipe)
# ============================================================================
# Detects segments based on changes in person count on stage
# Useful for shows with host introductions

transition_detection:
  enabled: true

  # Transition rules: Define what person count changes trigger segment boundaries
  # Format: [from_count, to_count, action]
  # Actions: "start_segment" or "end_segment"

  rules:
    # Host + Comedian → Comedian only (host exits)
    # START CLIPPING - Comedian is now alone on stage
    - from: 2
      to: 1
      action: start_segment
      description: "Host exits, comedian alone on stage"

    # Comedian only → Host + Comedian (host enters)
    # END CLIPPING - Someone joins the comedian
    - from: 1
      to: 2
      action: end_segment
      description: "Host enters, end comedian's solo set"

    # Empty stage → Person appears (comedian enters alone)
    - from: 0
      to: 1
      action: start_segment
      description: "Comedian enters empty stage"

    # Person on stage → Empty stage (comedian exits)
    - from: 1
      to: 0
      action: end_segment
      description: "Comedian exits, stage empty"

    # DISABLED: These broad rules interfere with the main 2→1 and 1→2 detection
    # They cause segments to start at time 0 and skip intermediate transitions
    # - from: any
    #   to: 2+
    #   action: start_segment
    #   description: "Multiple people on stage"

    # - from: 2+
    #   to: 1-
    #   action: end_segment
    #   description: "People exit stage"

  # Minimum frames with new count before triggering transition (prevents flicker)
  # Higher value = more stable but slower to detect real transitions
  # At 1 frame/sec sampling: 2 = 2 seconds, 3 = 3 seconds, 5 = 5 seconds
  # Note: With median smoothing enabled, lower values work well
  # LOWERED to 2 to catch brief transitions that get interrupted by 3rd person detections
  transition_stability_frames: 2  # Catch transitions even if count changes quickly

  # Confidence: How to combine face and pose counts
  # Options:
  #   "min" - min(faces, poses) - conservative
  #   "max" - max(faces, poses) - liberal (default for MediaPipe)
  #   "average" - average of faces and poses
  #   "yolo" - use YOLO person count (requires yolo_detection.enabled)
  #   "yolo_zone" - use YOLO count inside stage boundary only (requires zone_crossing.enabled)
  #   "hybrid" - prefer YOLO if available, fallback to max(faces, poses)
  person_count_method: yolo_zone  # Changed from yolo to exclude audience

  # Person count smoothing: Apply median filter to reduce detection noise
  # Helps handle inconsistent face detection (e.g., 88% vs 99% pose detection)
  person_count_smoothing: true
  smoothing_window: 7  # Balanced median filter - reduces noise while preserving real transitions

# ============================================================================
# POSITION-BASED DETECTION (Fallback Mode)
# ============================================================================
# Falls back to position-based detection if no transitions found
# Detects when person exits stage left/right

position_detection:
  enabled: true

  # Stage boundary thresholds (0.0 to 0.5)
  # 0.15 = person is within 15% of frame edge
  exit_threshold: 0.15

  # Position must be stable for N frames before triggering exit
  exit_stability_frames: 2

  # Track position of:
  # Options: "center" (bounding box center), "torso" (pose landmarks)
  tracking_point: torso

# ============================================================================
# YOLO PERSON DETECTION
# ============================================================================
# YOLOv8-based person detection for reliable multi-person tracking
# More accurate than MediaPipe Face Detection for detecting 2+ people

yolo_detection:
  # Enable YOLO detector (requires ultralytics package)
  enabled: true

  # YOLO model to use
  # Options: yolov8n.pt (nano, fastest), yolov8s.pt (small), yolov8m.pt (medium)
  model: "yolov8n.pt"

  # Detection confidence threshold (0.0-1.0)
  confidence: 0.5

  # Maximum frames an object can disappear before deregistration
  # Higher = more persistent tracking across occlusions
  max_disappeared_frames: 30

  # Velocity-based exit detection parameters
  velocity_window: 5  # Number of frames to use for velocity calculation
  exit_velocity_threshold: 20.0  # Minimum velocity (pixels/frame) toward edge for exit detection

# ============================================================================
# ZONE CROSSING DETECTION
# ============================================================================
# Define stage boundaries and detect when people enter/exit
# Works best with YOLO detection enabled

zone_crossing:
  # Enable zone crossing detection
  enabled: true  # ENABLED to exclude audience in foreground

  # Stage boundary definition
  stage_boundary:
    # Type: "rectangle", "polygon", or "ellipse"
    type: "rectangle"

    # For rectangle: specify bounds (in pixels or as fraction of frame if < 1.0)
    # These values define the stage area
    # ADJUSTED FOR FOREGROUND AUDIENCE: excludes bottom 15% where audience heads appear
    left: 0.05   # 5% from left edge (wide to include full stage)
    right: 0.95  # 95% from left edge (5% from right)
    top: 0.0     # Start from very top
    bottom: 0.85 # Stop at 85% down (excludes bottom 15% where audience is)

    # For polygon: list of [x, y] points defining the boundary (normalized 0-1)
    # Example: [[0.1, 0.1], [0.9, 0.1], [0.9, 0.85], [0.1, 0.85]]
    points: []

    # For ellipse: center and size
    ellipse_center: [0.5, 0.425]  # Center at 50% width, 42.5% height
    ellipse_width: 0.9  # 90% of frame width
    ellipse_height: 0.85  # 85% of frame height

  # Buffer zone (pixels) to prevent double-counting from position wobbling
  buffer_zone: 50

  # Minimum frames person must be inside/outside before counting as stable
  min_frames_inside: 3

  # Auto-calibration settings
  auto_calibrate:
    enabled: false  # Enable automatic zone calibration from first few minutes
    calibration_duration: 120  # Seconds to observe before calibrating
    min_samples: 100  # Minimum position samples needed
    confidence_threshold: 0.7  # Minimum confidence for auto-calibrated zones

# ============================================================================
# DETECTION CONFIDENCE
# ============================================================================

confidence:
  # MediaPipe Face Detection
  face_detection:
    min_detection_confidence: 0.5
    model_selection: 1  # 0=short range (< 2m), 1=full range (> 2m)

  # MediaPipe Pose Detection
  pose_detection:
    min_detection_confidence: 0.5
    min_tracking_confidence: 0.5
    model_complexity: 1  # 0=lite, 1=full, 2=heavy
    smooth_landmarks: true

# ============================================================================
# KALMAN FILTER (Smoothing)
# ============================================================================

kalman_filter:
  enabled: true

  # Process noise (how much we trust the model)
  # Lower = smoother but slower to adapt
  process_noise: 1.0

  # Measurement noise (how much we trust measurements)
  # Lower = trust measurements more
  measurement_noise: 5.0

  # Initial uncertainty
  initial_covariance: 10.0

# ============================================================================
# BLUR DETECTION
# ============================================================================
# Prevents clipping at blurry/out-of-focus frames
# Uses Laplacian variance to detect sharpness

blur_detection:
  enabled: false  # Disabled for better segment detection (re-enable if clips cut on blurry frames)

  # Blur threshold (Laplacian variance)
  # Lower = more strict (more frames considered blurry)
  # Typical values: 50-200
  threshold: 100.0

  # When segment boundary is blurry, shift to nearest sharp frame
  # Maximum frames to search (in both directions)
  boundary_shift_max_frames: 30  # ~1 second at 30fps

  # Minimum sharpness required for segment boundaries
  # Set higher than general threshold for better clip quality
  boundary_sharpness_min: 150.0

  # Log blurry frames to console
  log_blurry_frames: false

# ============================================================================
# SEGMENT FILTERING
# ============================================================================

filtering:
  # Minimum segment duration (seconds)
  min_duration: 15.0  # 15 seconds minimum to capture shorter segments while filtering noise

  # Maximum segment duration (seconds, 0 = no limit)
  max_duration: 0

  # Minimum gap between segments (seconds)
  # Segments closer than this may be merged
  min_gap: 5.0

  # Auto-merge close segments
  merge_close_segments: false  # Disabled to keep separate comedian sets

  # Time buffer (seconds) to add before/after detected transitions
  # Ensures we don't clip too close to the transition point
  buffer_before_start: 10.0  # Start clip 10s before transition to capture context
  buffer_after_end: 10.0     # End clip 10s after transition to capture outro

# ============================================================================
# VIDEO PROCESSING
# ============================================================================

processing:
  # Frame sampling rate
  # 1 = every frame, 2 = every other frame, etc.
  # Higher = faster but less accurate
  sample_rate: 30  # Process 1 frame per second (assuming 30fps)

  # Adaptive sampling: slow down when person count changes
  adaptive_sampling: true
  adaptive_rate_on_change: 10  # 3 frames per second during transitions for better capture

# ============================================================================
# OUTPUT
# ============================================================================

output:
  # Timestamped folder names
  use_timestamps: true

  # Folder naming
  clips_folder_suffix: "clips"
  debug_folder_suffix: "debug"

  # FFmpeg encoding settings
  ffmpeg:
    video_codec: libx264
    audio_codec: aac
    preset: fast  # ultrafast, superfast, veryfast, faster, fast, medium, slow, slower, veryslow
    crf: 23  # Quality (18-28, lower = better quality)

# ============================================================================
# DEBUG
# ============================================================================

debug:
  # Export debug frames
  export_frames: true

  # Which frames to export
  export_first_frame: true
  export_last_frame: true
  export_transition_frames: false  # Export frames where transitions occur

  # Overlay elements
  overlays:
    draw_pose_landmarks: true
    draw_face_boxes: true
    draw_stage_boundaries: true
    draw_position_indicator: true
    draw_text_info: true

  # Text info to display
  text_info:
    - frame_number
    - num_faces
    - num_poses
    - person_count
    - confidence_score
    - position
    - status

# ============================================================================
# EXPERIMENTAL
# ============================================================================

experimental:
  # Use motion detection to identify active speaker
  motion_based_segmentation: false

  # Use audio levels to detect applause/laughter
  audio_based_segmentation: false

  # Multi-person pose tracking (requires different model)
  multi_person_tracking: false
